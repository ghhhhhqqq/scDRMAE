{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import torch\n",
    "import random\n",
    "import anndata\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from datasets import Loader, apply_noise\n",
    "from model import scDRMAE\n",
    "from evaluate import evaluate\n",
    "from util import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset1( File1 = None, File2 = None, File3 = None, File4 = None, transpose = True, test_size_prop = None, state = 0,\n",
    "                  format_rna = None, formar_epi = None ):\n",
    "    # read single-cell multi-omics data together\n",
    "\n",
    "    ### raw reads count of scRNA-seq data\n",
    "    adata = adata1 = None\n",
    "\n",
    "    if File1 is not None:\n",
    "        if format_rna == \"table\":\n",
    "            adata  = sc.read(File1)\n",
    "        else: # 10X format\n",
    "            adata  = sc.read_mtx(File1)\n",
    "\n",
    "        if transpose:\n",
    "            adata  = adata.transpose()\n",
    "\n",
    "    ##$ the binarization data for scEpigenomics file\n",
    "    if File2 is not None:\n",
    "        if formar_epi == \"table\":\n",
    "            adata1  = sc.read( File2 )\n",
    "        else  :# 10X format\n",
    "            adata1  = sc.read_mtx(File2)\n",
    "\n",
    "        if transpose:\n",
    "            adata1  = adata1.transpose()\n",
    "\n",
    "    ### File3 and File4 for cell group information of scRNA-seq and scEpigenomics data\n",
    "    label_ground_truth = []\n",
    "    label_ground_truth1 = []\n",
    "\n",
    "    if state == 0 :\n",
    "        if File3 is not None:\n",
    "            Data2  = pd.read_csv( File3, header=0, index_col=0 )\n",
    "            label_ground_truth =  Data2['trueType_y'].values\n",
    "\n",
    "        else:\n",
    "            label_ground_truth =  np.ones( len( adata.obs_names ) )\n",
    "\n",
    "        if File4 is not None:\n",
    "            Data2 = pd.read_csv( File4, header=0, index_col=0 )\n",
    "            label_ground_truth1 = Data2['trueType_y'].values\n",
    "\n",
    "        else:\n",
    "            label_ground_truth1 =  np.ones( len( adata.obs_names ) )\n",
    "\n",
    "    elif state == 1:\n",
    "        if File3 is not None:\n",
    "            Data2 = pd.read_table( File3, header=0, index_col=0 )\n",
    "            label_ground_truth = Data2['cell_line'].values\n",
    "        else:\n",
    "            label_ground_truth =  np.ones( len( adata.obs_names ) )\n",
    "\n",
    "        if File4 is not None:\n",
    "            Data2 = pd.read_table( File4, header=0, index_col=0 )\n",
    "            label_ground_truth1 = Data2['cell_line'].values\n",
    "        else:\n",
    "            label_ground_truth1 =  np.ones( len( adata.obs_names ) )\n",
    "\n",
    "    elif state == 3:\n",
    "        if File3 is not None:\n",
    "            Data2 = pd.read_table( File3, header=0, index_col=0 )\n",
    "            label_ground_truth = Data2['Group'].values\n",
    "        else:\n",
    "            label_ground_truth =  np.ones( len( adata.obs_names ) )\n",
    "\n",
    "        if File4 is not None:\n",
    "            Data2 = pd.read_table( File4, header=0, index_col=0 )\n",
    "            label_ground_truth1 = Data2['Group'].values\n",
    "        else:\n",
    "            label_ground_truth1 =  np.ones( len( adata.obs_names ) )\n",
    "\n",
    "    else:\n",
    "        if File3 is not None:\n",
    "            Data2 = pd.read_table( File3, header=0, index_col=0 )\n",
    "            label_ground_truth = Data2['Cluster'].values\n",
    "        else:\n",
    "            label_ground_truth =  np.ones( len( adata.obs_names ) )\n",
    "\n",
    "        if File4 is not None:\n",
    "            Data2 = pd.read_table( File4, header=0, index_col=0 )\n",
    "            label_ground_truth1 = Data2['Cluster'].values\n",
    "        else:\n",
    "            label_ground_truth1 =  np.ones( len( adata.obs_names ) )\n",
    "\n",
    "    # split datasets into training and testing sets\n",
    "    if test_size_prop > 0 :\n",
    "        train_idx, test_idx = train_test_split(np.arange(adata.n_obs),\n",
    "                                               test_size = test_size_prop,\n",
    "                                               random_state = 200)\n",
    "        spl = pd.Series(['train'] * adata.n_obs)\n",
    "        spl.iloc[test_idx]  = 'test'\n",
    "        adata.obs['split']  = spl.values\n",
    "\n",
    "        if File2 is not None:\n",
    "            adata1.obs['split'] = spl.values\n",
    "    else:\n",
    "        train_idx, test_idx = list(range( adata.n_obs )), list(range( adata.n_obs ))\n",
    "        spl = pd.Series(['train'] * adata.n_obs)\n",
    "        adata.obs['split']       = spl.values\n",
    "\n",
    "        if File2 is not None:\n",
    "            adata1.obs['split']  = spl.values\n",
    "\n",
    "    adata.obs['split'] = adata.obs['split'].astype('category')\n",
    "    adata.obs['Group'] = label_ground_truth\n",
    "    adata.obs['Group'] = adata.obs['Group'].astype('category')\n",
    "\n",
    "    if File2 is not None:\n",
    "        adata1.obs['split'] = adata1.obs['split'].astype('category')\n",
    "        adata1.obs['Group'] = label_ground_truth\n",
    "        adata1.obs['Group'] = adata1.obs['Group'].astype('category')\n",
    "\n",
    "    return adata, adata1, train_idx, test_idx, label_ground_truth, label_ground_truth1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(count_mat): \n",
    "    \"\"\"\n",
    "    TF-IDF transformation for matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    count_mat\n",
    "        numpy matrix with cells as rows and peak as columns, cell * peak.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tfidf_mat\n",
    "        matrix after TF-IDF transformation.\n",
    "\n",
    "    divide_title\n",
    "        matrix divided in TF-IDF transformation process, would be used in \"inverse_TFIDF\".\n",
    "\n",
    "    multiply_title\n",
    "        matrix multiplied in TF-IDF transformation process, would be used in \"inverse_TFIDF\".\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    count_mat = count_mat.T\n",
    "    divide_title = np.tile(np.sum(count_mat,axis=0), (count_mat.shape[0],1))\n",
    "    nfreqs = 1.0 * count_mat / divide_title\n",
    "    multiply_title = np.tile(np.log(1 + 1.0 * count_mat.shape[1] / np.sum(count_mat,axis=1)).reshape(-1,1), (1,count_mat.shape[1]))\n",
    "    tfidf_mat = scipy.sparse.csr_matrix(np.multiply(nfreqs, multiply_title)).T\n",
    "    return tfidf_mat, divide_title, multiply_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize(adata, filter_min_counts=True, size_factors=True, normalize_input=True, logtrans_input=True,\n",
    "              filter_gene=0.01,datatype='RNA'):\n",
    "    ##过滤掉在低于1%的细胞中表达的特征\n",
    "    if filter_gene:\n",
    "       adata = adata[:, (adata.X > 0).sum(0) >= adata.shape[0]*0.01] \n",
    "       \n",
    "    if size_factors or normalize_input or logtrans_input:\n",
    "        adata.raw = adata.copy()\n",
    "    else:\n",
    "        adata.raw = adata\n",
    "    if datatype=='ATAC':\n",
    "        count_mat = adata.X.copy()\n",
    "        adata.X, divide_title, multiply_title = TFIDF(count_mat)\n",
    "        max_temp = np.max(adata.X)\n",
    "        adata.X = adata.X / max_temp\n",
    "        # return adata, divide_title, multiply_title\n",
    "    if size_factors:\n",
    "        sc.pp.normalize_per_cell(adata)\n",
    "        adata.obs['size_factors'] = adata.obs.n_counts / np.median(adata.obs.n_counts)\n",
    "    else:\n",
    "        adata.obs['size_factors'] = 1.0\n",
    "\n",
    "    if logtrans_input:\n",
    "        sc.pp.log1p(adata)\n",
    "\n",
    "    if normalize_input:\n",
    "        sc.pp.scale(adata)\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(directory_path, new_folder_name):\n",
    "    \"\"\"Creates an expected directory if it does not exist\"\"\"\n",
    "    directory_path = os.path.join(directory_path, new_folder_name)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    return directory_path\n",
    "\n",
    "\n",
    "def inference(net, data_loader_test):\n",
    "    net.eval()\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    with torch.no_grad():\n",
    "        for step, (x,x1, y) in enumerate(data_loader_test):\n",
    "            feature_vector.extend(net.feature(x.to(device).float(), x1.to(device).float()).detach().cpu().numpy())\n",
    "            labels_vector.extend(y.numpy())\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "    return feature_vector, labels_vector\n",
    "def chabu1(net, data_loader_test):\n",
    "    net.eval()\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    with torch.no_grad():\n",
    "        for step, (x,x1, y) in enumerate(data_loader_test):\n",
    "            feature_vector.extend(net.chabu(x.to(device).float(), x1.to(device).float())[0].detach().cpu().numpy())\n",
    "            labels_vector.extend(y.numpy())\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "    return feature_vector, labels_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_search_fixed_clus(adata, fixed_clus_count, increment=0.02):\n",
    "    '''\n",
    "        arg1(adata)[AnnData matrix]\n",
    "        arg2(fixed_clus_count)[int]\n",
    "\n",
    "        return:\n",
    "            resolution[int]\n",
    "    '''\n",
    "    dis = []\n",
    "    resolutions = sorted(list(np.arange(0.01, 2.5, increment)), reverse=True)\n",
    "    i = 0\n",
    "    res_new = []\n",
    "    for res in resolutions:\n",
    "        sc.tl.leiden(adata, random_state=0, resolution=res)\n",
    "        count_unique_leiden = len(pd.DataFrame(\n",
    "            adata.obs['leiden']).leiden.unique())\n",
    "        dis.append(abs(count_unique_leiden-fixed_clus_count))\n",
    "        res_new.append(res)\n",
    "        if count_unique_leiden == fixed_clus_count:\n",
    "            break\n",
    "    reso = resolutions[np.argmin(dis)]\n",
    "\n",
    "    return reso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['batch_size'] = 256\n",
    "args['n_classes'] = 4\n",
    "args['dataset'] = 'InHouse'\n",
    "if args['dataset'] in ['InHouse']:\n",
    "    args['epochs'] = 20\n",
    "    args[\"learning_rate\"] = 0.001\n",
    "elif args['dataset'] in ['human cell line mixture']:\n",
    "    args['epochs'] = 50\n",
    "    args[\"learning_rate\"] = 0.001\n",
    "elif args['dataset'] in ['human cell line mixture']:\n",
    "    args['epochs'] = 20\n",
    "    args[\"learning_rate\"] = 0.002\n",
    "else:\n",
    "    args['epochs'] = 100\n",
    "    args[\"learning_rate\"] = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/hfzhang/workplace/scMCs/scMCs/data'\n",
    "X1 = os.path.join('/home/hfzhang/workplace/scDRMAE/data/GSM4476364_RNA_raw.csv.gz')\n",
    "X2 = os.path.join('/home/hfzhang/workplace/scDRMAE/data/GSM4476364_ADT_raw.csv.gz')\n",
    "x3 = os.path.join('/home/hfzhang/workplace/scDRMAE/data/truth_InHouse.csv') # cell type information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/anndata/_core/anndata.py:1838: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "x1, x2, train_index, test_index, label_ground_truth, _ = read_dataset1(File1=X1, File2=X2,\n",
    "                                                                          File3=x3, File4=None,\n",
    "                                                                          transpose=True, test_size_prop=0.0,\n",
    "                                                                          state=0, format_rna=\"table\",\n",
    "                                                                          formar_epi=\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.obs['cell_type']=label_ground_truth\n",
    "x2.obs['cell_type']=label_ground_truth\n",
    "x1.obs['cell_type'] = x1.obs['cell_type'].astype('category')\n",
    "x2.obs['cell_type'] = x2.obs['cell_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, labels = np.unique(label_ground_truth, return_inverse=True)\n",
    "classes = classes.tolist()\n",
    "args['n_classes'] = len(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1[:, (x1.X > 0).sum(0) >= x1.shape[0]*0.01] \n",
    "x2 = x2[:, (x2.X > 0).sum(0) >= x2.shape[0]*0.01] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/scanpy/preprocessing/_normalization.py:169: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "# 归一化，使得不同细胞样本间可比\n",
    "sc.pp.normalize_total(x1, target_sum=1e4)\n",
    "sc.pp.log1p(x1)\n",
    "sc.pp.highly_variable_genes(x1, n_top_genes=3000)\n",
    "x_scRNA = x1[:, x1.var['highly_variable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/scanpy/preprocessing/_normalization.py:169: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    }
   ],
   "source": [
    "# 归一化，使得不同细胞样本间可比\n",
    "sc.pp.normalize_total(x2, target_sum=1e4)\n",
    "sc.pp.log1p(x2)\n",
    "sc.pp.highly_variable_genes(x2, n_top_genes=3000)\n",
    "x_scATAC = x2[:, x2.var['highly_variable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scRNA=normalize(x_scRNA, filter_min_counts=False, size_factors=False, normalize_input=True, logtrans_input=False,\n",
    "               filter_gene=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scATAC=normalize(x_scATAC,filter_gene=0, filter_min_counts=False, size_factors=False, normalize_input=True, logtrans_input=False,datatype='AAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ground_truth=x_scRNA.obs['cell_type'].values\n",
    "classes, labels = np.unique(label_ground_truth, return_inverse=True)\n",
    "classes = classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['n_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBMC():\n",
    "    def __init__(self, rna, atac, cell_type):\n",
    "        self.rna_data = rna  # 读取scRNA-seq数据\n",
    "        self.atac_data = atac  # 读取scATAC-seq数据\n",
    "        self.cell_type = cell_type  # 读取细胞类型标签\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rna_data)  # 返回数据集的长度\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rna_sample = self.rna_data[idx]  # 获取scRNA-seq样本\n",
    "        atac_sample = self.atac_data[idx]  # 获取scATAC-seq样本\n",
    "        cell_type_label = self.cell_type[idx]  # 获取细胞类型标签\n",
    "\n",
    "        return rna_sample, atac_sample, cell_type_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_epoch 8\n",
      "best_ari 0.9476969307916453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttest: [nmi: 0.951458] [ari: 0.975006] [ami: 0.949422]\n",
      "best_epoch 18\n",
      "best_ari 0.9592236690668591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttest: [nmi: 0.940318] [ari: 0.935737] [ami: 0.938043]\n",
      "best_epoch 7\n",
      "best_ari 0.9614096892955325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttest: [nmi: 0.933087] [ari: 0.921679] [ami: 0.930495]\n",
      "best_epoch 8\n",
      "best_ari 0.9502687482296138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttest: [nmi: 0.956394] [ari: 0.969171] [ami: 0.954715]\n",
      "best_epoch 15\n",
      "best_ari 0.9532722971185495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hfzhang/software/anaconda3/envs/scEMC/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttest: [nmi: 0.945497] [ari: 0.962437] [ami: 0.943286]\n",
      "nmi为0.9453506924231665,ari为0.9528059443926515,ami0.9431922043078863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import KMeans\n",
    "import anndata\n",
    "crossFold=5\n",
    "kf = KFold(n_splits=crossFold, shuffle=True, random_state=42)\n",
    "nmis_test=[]\n",
    "aris_test=[]\n",
    "amis_test=[]\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(x_scRNA)):\n",
    "    con_loss=[]\n",
    "    re_loss=[]\n",
    "    results=[]\n",
    "    x_scRNA_train, x_scRNA_test = x_scRNA[train_indices], x_scRNA[val_indices]\n",
    "    x_scATAC_train,x_scATAC_test =x_scATAC[train_indices], x_scATAC[val_indices]\n",
    "    x_scRNA_train_aug, x_scATAC_train_aug=x_scRNA_train, x_scATAC_train\n",
    "\n",
    "    y_train=labels[train_indices]\n",
    "    y_test=labels[val_indices]\n",
    "\n",
    "    x_scRNA_train=torch.from_numpy(x_scRNA_train_aug.X.toarray())\n",
    "    x_scATAC_train=torch.from_numpy(x_scATAC_train_aug.X.toarray())\n",
    "    x_scRNA_test=torch.from_numpy(x_scRNA_test.X.toarray())\n",
    "    x_scATAC_test=torch.from_numpy(x_scATAC_test.X.toarray())\n",
    "    x_scATAC_all=torch.from_numpy(x_scATAC.X)\n",
    "    x_scRNA_all=torch.from_numpy(x_scRNA.X)\n",
    "\n",
    "    N1_train, M1_train = np.shape(x_scRNA_train)\n",
    "    N2_train, M2_train = np.shape(x_scATAC_train)\n",
    "    N1_test, M1_test = np.shape(x_scRNA_test)\n",
    "    N2_test, M2_test = np.shape(x_scATAC_test)\n",
    "    ##训练\n",
    "    dataset_train = PBMC(x_scRNA_train,x_scATAC_train,y_train)\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    dataset = PBMC(x_scRNA_all,x_scATAC_all,labels)\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    data_loader_all = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    dataset_test = PBMC(x_scRNA_test,x_scATAC_test,y_test)\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    dims = [M1_train, M2_train]\n",
    "    view = 2\n",
    "    data_size = len(label_ground_truth)\n",
    "    data_size_train = len(y_train)\n",
    "    data_size_test = len(y_test)\n",
    "    class_num = len(classes)\n",
    "    init_lr = args[\"learning_rate\"]\n",
    "    count=0\n",
    "    max_epochs = args['epochs']\n",
    "    mask_probas = [0.4]*M1_train\n",
    "    mask_probas1 = [0.4]*M2_train\n",
    "    model = scDRMAE(\n",
    "                num_genes=M1_train,\n",
    "                num_ATAC=M2_train,\n",
    "                hidden_size=128,\n",
    "                masked_data_weight=0.75,\n",
    "                mask_loss_weight=0.7\n",
    "            ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "\n",
    "    if not os.path.exists('./models'):\n",
    "        os.makedirs('./models')\n",
    "    best_ari=0\n",
    "    best_nmi=0\n",
    "    best_epoch=0\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        meter = AverageMeter()\n",
    "        for i, (x,x1, y) in enumerate(data_loader_train):\n",
    "            x=x.float()\n",
    "            x1=x1.float()\n",
    "            x = x.to(device)\n",
    "            x1 = x1.to(device)\n",
    "            x_corrputed, mask = apply_noise(x, mask_probas)\n",
    "            x_corrputed1, mask1 = apply_noise(x1, mask_probas1)\n",
    "            optimizer.zero_grad()\n",
    "            x_corrputed_latent, loss = model.loss_mask(x_corrputed, x, mask,x_corrputed1, x1, mask1,epoch,max_epochs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            meter.update(loss.detach().cpu().numpy())\n",
    "\n",
    "        latent, true_label = inference(model, data_loader_val)\n",
    "        clustering_model = KMeans(n_clusters=args[\"n_classes\"],n_init=6)\n",
    "        clustering_model.fit(latent)\n",
    "        pred_label = clustering_model.labels_\n",
    "            \n",
    "        nmi, ari, acc,ami = evaluate(true_label, pred_label)\n",
    "        ss = silhouette_score(latent, pred_label)\n",
    "        if ari>best_ari:\n",
    "            best_ari=ari\n",
    "            best_epoch=epoch\n",
    "            best_state=model.state_dict()\n",
    "\n",
    "\n",
    "    print('best_epoch',best_epoch)\n",
    "    print('best_ari',best_ari)\n",
    "    model.load_state_dict(best_state)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        latent, true_label = inference(model,\n",
    "                                        data_loader_test)\n",
    "        clustering_model = KMeans(n_clusters=args[\"n_classes\"])\n",
    "        clustering_model.fit(latent)\n",
    "        pred_label = clustering_model.labels_\n",
    "        nmi_test, ari_test, acc_test, ami_test = evaluate(true_label, pred_label)\n",
    "        print(\"\\ttest: [nmi: %f] [ari: %f] [ami: %f]\" % (nmi_test, ari_test, ami_test))\n",
    "\n",
    "    aris_test.append(ari_test)\n",
    "    nmis_test.append(nmi_test)\n",
    "    amis_test.append(ami_test)\n",
    "nmi=sum(nmis_test)/(fold+1)\n",
    "ari=sum(aris_test)/(fold+1)\n",
    "ami=sum(amis_test)/(fold+1)\n",
    "print('nmi为{},ari为{},ami{}'.format(nmi,ari,ami))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
